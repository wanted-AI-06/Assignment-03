{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS_base_line_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# base_line 모델(roberta-base)의 선정 및 f1_score & pearsonr\n",
        "\n",
        "Team LostCow의 roberta모델을 기준으로 함\n",
        "\n",
        "출처: https://github.com/l-yohai/KLUE\n",
        "\n",
        "pretrained_model : klue/roberta-base()\n",
        "\n",
        "tokenizer : klue/roberta-base\n",
        "\n",
        "dropout : 0.1\n",
        "\n",
        "batch_size : 64\n",
        "\n",
        "optim /learning_rate : adamw_hf / 5e-5\n",
        "\n",
        "loss_fnc : MSELoss"
      ],
      "metadata": {
        "id": "pmsQRpsmRRfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez-KaYpLFd04",
        "outputId": "309f9c22-cd52-4cdc-abc7-007bf380f40f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 30.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 19.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 21.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 34.0 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "#훈련 데이터셋 다운로드 및 압축풀기\n",
        "\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
        "\n",
        "tar_bz2_file = tarfile.open(\"/content/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=\"/content\")\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "3J5pSUgIHv0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c9e5b1-51fc-4be3-f6f5-74740db0ae70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 07:12:50--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.218.169.187\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.218.169.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-03-17 07:12:50 (16.4 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#GPU 사용 여부 확인 및 name 확인\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "oZoPB1WKJnet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a528e5b9-bab5-4101-9bb6-499aff9b8461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jy5VuDP_w1T",
        "outputId": "04a5bb09-7fbd-4888-f084-2b04317cc8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "\n",
        "train_length=int(len(data)*0.9)\n",
        "train=data[:train_length]\n",
        "vaild=data[train_length:]\n",
        "\n",
        "print('data_set:',len(data))\n",
        "print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "with open('train_split.json','w') as f:\n",
        "  json.dump(train,f,ensure_ascii = False)\n",
        "with open('valid_split.json','w') as f:\n",
        "  json.dump(vaild,f,ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/l-yohai/KLUE #base line 모델 불러오기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7hnyZVgAuf2",
        "outputId": "1bf2b578-e15e-4eb0-c9bf-934f753a9d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KLUE'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 313 (delta 52), reused 101 (delta 29), pack-reused 179\u001b[K\n",
            "Receiving objects: 100% (313/313), 5.79 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# base line 모델 학습\n",
        "!python /content/KLUE/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvAb8SjFdMR",
        "outputId": "48012329-1d4e-474e-cd6b-0b07f3b11a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['cls_fc_layer.linear.weight', 'sentence_fc_layer.linear.bias', 'dense.linear.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'label_classifier.linear.weight', 'label_classifier.linear.bias', 'sentence_fc_layer.linear.weight', 'cls_fc_layer.linear.bias', 'dense.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.1313, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:14<19:13,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.77it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.76it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.22365519404411316, 'eval_pearsonr': 0.9507152306522215, 'eval_runtime': 4.5377, 'eval_samples_per_second': 257.181, 'eval_steps_per_second': 4.187, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:18<19:13,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.2105, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:37<17:57,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5125826001167297, 'eval_pearsonr': 0.946132521751175, 'eval_runtime': 4.6456, 'eval_samples_per_second': 251.204, 'eval_steps_per_second': 4.09, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:41<17:57,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.1673, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:01<16:46,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.16780120134353638, 'eval_pearsonr': 0.9623197233976968, 'eval_runtime': 4.558, 'eval_samples_per_second': 256.031, 'eval_steps_per_second': 4.168, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:05<16:46,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1224, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:24<15:28,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2284928411245346, 'eval_pearsonr': 0.9581630776284613, 'eval_runtime': 4.5058, 'eval_samples_per_second': 259.001, 'eval_steps_per_second': 4.217, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:28<15:28,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1131, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:47<13:36,  1.41it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2404332458972931, 'eval_pearsonr': 0.95887289742035, 'eval_runtime': 4.5079, 'eval_samples_per_second': 258.88, 'eval_steps_per_second': 4.215, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:52<13:36,  1.41it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.0858, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:11<12:59,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.26666244864463806, 'eval_pearsonr': 0.9607266825802501, 'eval_runtime': 4.7416, 'eval_samples_per_second': 246.118, 'eval_steps_per_second': 4.007, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:16<12:59,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0767, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:34<11:45,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.26778799295425415, 'eval_pearsonr': 0.960294526518308, 'eval_runtime': 4.5198, 'eval_samples_per_second': 258.2, 'eval_steps_per_second': 4.204, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:39<11:45,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0596, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [10:58<10:31,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.55it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.22287163138389587, 'eval_pearsonr': 0.9585569179329159, 'eval_runtime': 4.5415, 'eval_samples_per_second': 256.963, 'eval_steps_per_second': 4.184, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:03<10:31,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.054, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:21<09:16,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2512325346469879, 'eval_pearsonr': 0.9604608687973275, 'eval_runtime': 4.5231, 'eval_samples_per_second': 258.008, 'eval_steps_per_second': 4.201, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:26<09:16,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0491, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:44<07:59,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.46it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2536238729953766, 'eval_pearsonr': 0.9605980182317052, 'eval_runtime': 4.6591, 'eval_samples_per_second': 250.478, 'eval_steps_per_second': 4.078, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:49<07:59,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0366, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:08<06:49,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.18993763625621796, 'eval_pearsonr': 0.9603322574005452, 'eval_runtime': 4.5355, 'eval_samples_per_second': 257.301, 'eval_steps_per_second': 4.189, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:13<06:49,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.035, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:32<05:34,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.251998633146286, 'eval_pearsonr': 0.9609871499442095, 'eval_runtime': 4.523, 'eval_samples_per_second': 258.012, 'eval_steps_per_second': 4.201, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:36<05:34,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 0.0306, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [17:55<04:19,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2188197672367096, 'eval_pearsonr': 0.9598166926080405, 'eval_runtime': 4.5463, 'eval_samples_per_second': 256.691, 'eval_steps_per_second': 4.179, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:00<04:19,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0267, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:19<03:05,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21170051395893097, 'eval_pearsonr': 0.9609987739515057, 'eval_runtime': 4.6494, 'eval_samples_per_second': 250.997, 'eval_steps_per_second': 4.087, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:23<03:05,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.0242, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:42<01:51,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21484673023223877, 'eval_pearsonr': 0.9624502743855923, 'eval_runtime': 4.5195, 'eval_samples_per_second': 258.212, 'eval_steps_per_second': 4.204, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:46<01:51,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.0216, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:06<00:37,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23112452030181885, 'eval_pearsonr': 0.9630249943800897, 'eval_runtime': 4.5336, 'eval_samples_per_second': 257.41, 'eval_steps_per_second': 4.191, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:10<00:37,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "100% 1650/1650 [22:51<00:00,  1.81it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1600 (score: 0.9630249943800897).\n",
            "{'train_runtime': 1372.3942, 'train_samples_per_second': 76.516, 'train_steps_per_second': 1.202, 'train_loss': 0.1366526355165424, 'epoch': 10.0}\n",
            "100% 1650/1650 [22:52<00:00,  1.20it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/klue-sts-v1.1.tar.gz /content/model/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "id": "8tWlmn5hONdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 base line 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/KLUE/sts/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "P_WMvlynKy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]"
      ],
      "metadata": {
        "id": "94-yxLA9Ny2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pandas as pd\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>3.1).astype(int)\n",
        "print(\"epochs 10 / >3.1\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsHBuMmMUYRb",
        "outputId": "a60a662c-f453-4802-f78b-c0da075b5b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.66      0.79       299\n",
            "           1       0.68      1.00      0.81       220\n",
            "\n",
            "    accuracy                           0.80       519\n",
            "   macro avg       0.84      0.83      0.80       519\n",
            "weighted avg       0.86      0.80      0.80       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8103130755064457\n",
            "\n",
            "epochs 10 / >3.1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.80      0.87       299\n",
            "           1       0.78      0.96      0.86       220\n",
            "\n",
            "    accuracy                           0.87       519\n",
            "   macro avg       0.87      0.88      0.87       519\n",
            "weighted avg       0.89      0.87      0.87       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.859470468431772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnSRP83NiuA2",
        "outputId": "ff97efc1-8594-4f48-c29e-a445ca92f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8939478658048167}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델(roberta-base)의 custom 및 f1_score & pearsonr\n",
        "\n",
        "모델의 FClayer 층이 참고하는 roberta model의 hidden states를 다각화 하여 성능을 향상시켰습니다.\n",
        "\n",
        "base model로 선정한 LostCow team의 FClayer층의 학습 방식은 roberta model이 문장A,B에 대해서 token_type_ids 값을 입력하지 않고 구분되지 않은 상태로 학습시키며 마지막 layer의 임베딩벡터를 문장A,B의 위치를 마스킹해둔 s1_mask, s2_mask를 이용하여 분리한뒤 각 단어들의 dim의 위치값을 평균내어 하나의 토큰 임베딩 값으로 생성한 후 FClayer를 통해 연산됩니다.\n",
        "\n",
        "이 때문에 roberta model의 pooled_output을 사용하지 않으며 사용시에도 성능 향상에 기여하지 못합니다.\n",
        "\n",
        "각 문장의 임베딩 벡터의 평균값을 기준으로 FClayer의 연산이 이루어지는 점을 이용하여 각 문장의 의미가 적당하게 고려된 hidden_states layer를 탐색하여 2,3layer의 hidden_states를 추가로 concat하는 것으로 모델의 성능을 향상시켰습니다. "
      ],
      "metadata": {
        "id": "tdmvRwOznWua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"   \n",
        "\n",
        "  -------------------base model(model.forward code)----------------------\n",
        "        outputs = self.roberta(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=None\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        s1_h = self.entity_average(sequence_output, s1_mask)\n",
        "        s2_h = self.entity_average(sequence_output, s2_mask)\n",
        "        s1_h = self.sentence_fc_layer(s1_h)\n",
        "        s2_h = self.sentence_fc_layer(s2_h)\n",
        "\n",
        "        concat_h = torch.cat([s1_h, s2_h], dim=-1)\n",
        "        concat_h = self.dense(concat_h)\n",
        "\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      ----------------custom model(model.forward code)---------------------\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=None, output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        s1_h = self.entity_average(sequence_output, s1_mask)\n",
        "        s2_h = self.entity_average(sequence_output, s2_mask)\n",
        "        s1_h = self.sentence_fc_layer(s1_h)\n",
        "        s2_h = self.sentence_fc_layer(s2_h)\n",
        "        \n",
        "        sequence_output_sec_layer=outputs['hidden_states'][2]\n",
        "        ss1_h = self.entity_average(sequence_output_sec_layer, s1_mask)\n",
        "        ss2_h = self.entity_average(sequence_output_sec_layer, s2_mask)\n",
        "        ss1_h = self.sentence_fc_layer2(ss1_h)\n",
        "        ss2_h = self.sentence_fc_layer2(ss2_h)\n",
        "\n",
        "        sequence_output_sec_layer2=outputs['hidden_states'][3]\n",
        "        sss1_h = self.entity_average(sequence_output_sec_layer2, s1_mask)\n",
        "        sss2_h = self.entity_average(sequence_output_sec_layer2, s2_mask)\n",
        "        sss1_h = self.sentence_fc_layer3(sss1_h)\n",
        "        sss2_h = self.sentence_fc_layer3(sss2_h)\n",
        "\n",
        "        concat_h = torch.cat([s1_h, s2_h,ss1_h, ss2_h,sss1_h, sss2_h], dim=-1)\n",
        "        concat_h = self.dense(concat_h)\n",
        "        concat_h = self.dense2(concat_h)\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + ()\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JghKQehJn2iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 custom을 진행한 폴더 압축해제\n",
        "!unzip /content/KLUE_custom.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9giLSCmBnfBY",
        "outputId": "2401c465-c3d2-44a5-d77f-d723fdb79f7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/KLUE_custom.zip\n",
            "  inflating: sts/__pycache__/dataset.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/metric.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/model.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/utils.cpython-37.pyc  \n",
            "  inflating: sts/data/download.sh    \n",
            "  inflating: sts/README.md           \n",
            "  inflating: sts/dataloader.py       \n",
            "  inflating: sts/dataset.py          \n",
            "  inflating: sts/infer_test.ipynb    \n",
            "  inflating: sts/inference.py        \n",
            "  inflating: sts/metric.py           \n",
            "  inflating: sts/model.py            \n",
            " extracting: sts/requirements.txt    \n",
            "  inflating: sts/train.py            \n",
            "  inflating: sts/utils.py            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom 모델 학습\n",
        "!python /content/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPGQIl6onfi3",
        "outputId": "3ccd4f9e-d270-46a6-d078-bdc54931aa3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 546/546 [00:00<00:00, 502kB/s]\n",
            "Downloading: 100% 375/375 [00:00<00:00, 345kB/s]\n",
            "Downloading: 100% 243k/243k [00:00<00:00, 994kB/s]\n",
            "Downloading: 100% 734k/734k [00:00<00:00, 2.34MB/s]\n",
            "Downloading: 100% 173/173 [00:00<00:00, 158kB/s]\n",
            "Downloading: 100% 422M/422M [00:10<00:00, 42.7MB/s]\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['embedding_vectors.linear.bias', 'dense2.linear.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'sentence_fc_layer2.linear.weight', 'label_classifier.linear.weight', 'dense2.linear.bias', 'sentence_fc_layer2.linear.bias', 'embedding_vectors.linear.weight', 'sentence_fc_layer.linear.weight', 'label_classifier.linear.bias', 'dense.linear.bias', 'sentence_fc_layer3.linear.bias', 'cls_fc_layer.linear.bias', 'sentence_fc_layer3.linear.weight', 'cls_fc_layer.linear.weight', 'dense.linear.weight', 'sentence_fc_layer.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.1065, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:15<19:25,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.90it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.70it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.57it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 95% 18/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "\n",
            "Downloading builder script: 3.85kB [00:00, 2.72MB/s]       \n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.23697331547737122, 'eval_pearsonr': 0.9533520232740954, 'eval_runtime': 4.6316, 'eval_samples_per_second': 251.966, 'eval_steps_per_second': 4.102, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:19<19:25,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.2254, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:39<18:09,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.32829755544662476, 'eval_pearsonr': 0.9555207167369182, 'eval_runtime': 4.5634, 'eval_samples_per_second': 255.728, 'eval_steps_per_second': 4.164, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:44<18:09,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.179, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:04<16:54,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.44it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3492515981197357, 'eval_pearsonr': 0.9560363878093937, 'eval_runtime': 4.5578, 'eval_samples_per_second': 256.042, 'eval_steps_per_second': 4.169, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:08<16:54,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.13, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:28<15:41,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.34it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.22094199061393738, 'eval_pearsonr': 0.9610609854522894, 'eval_runtime': 4.7071, 'eval_samples_per_second': 247.923, 'eval_steps_per_second': 4.036, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:33<15:41,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1151, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:53<13:47,  1.39it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.69it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.91it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.70it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.25418609380722046, 'eval_pearsonr': 0.9582419991207989, 'eval_runtime': 4.5983, 'eval_samples_per_second': 253.79, 'eval_steps_per_second': 4.132, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:57<13:47,  1.39it/s]\n",
            "100% 19/19 [00:04<00:00,  4.33it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.089, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:18<13:09,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.37454137206077576, 'eval_pearsonr': 0.9596600659735962, 'eval_runtime': 4.5754, 'eval_samples_per_second': 255.059, 'eval_steps_per_second': 4.153, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:22<13:09,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0816, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:42<11:54,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.27998587489128113, 'eval_pearsonr': 0.9614724870042339, 'eval_runtime': 4.5887, 'eval_samples_per_second': 254.322, 'eval_steps_per_second': 4.141, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:47<11:54,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.0674, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:07<10:43,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.20271660387516022, 'eval_pearsonr': 0.9629096454955454, 'eval_runtime': 4.5641, 'eval_samples_per_second': 255.69, 'eval_steps_per_second': 4.163, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:11<10:43,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.057, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:31<09:24,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3146175444126129, 'eval_pearsonr': 0.9579630115130943, 'eval_runtime': 4.5704, 'eval_samples_per_second': 255.337, 'eval_steps_per_second': 4.157, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:36<09:24,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0527, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:56<08:06,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23704056441783905, 'eval_pearsonr': 0.9628089862283326, 'eval_runtime': 4.5464, 'eval_samples_per_second': 256.685, 'eval_steps_per_second': 4.179, 'epoch': 6.06}\n",
            " 61% 1000/1650 [14:00<08:06,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0424, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:21<06:54,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.20439551770687103, 'eval_pearsonr': 0.9632168067694619, 'eval_runtime': 4.5438, 'eval_samples_per_second': 256.833, 'eval_steps_per_second': 4.182, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:25<06:54,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0402, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:45<05:39,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23330500721931458, 'eval_pearsonr': 0.961794722151982, 'eval_runtime': 4.5333, 'eval_samples_per_second': 257.426, 'eval_steps_per_second': 4.191, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:50<05:39,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.0363, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:10<04:23,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.69it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.44it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.40it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2056594043970108, 'eval_pearsonr': 0.963302000959181, 'eval_runtime': 4.5788, 'eval_samples_per_second': 254.869, 'eval_steps_per_second': 4.15, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:14<04:23,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 0.0309, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:34<03:08,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21020977199077606, 'eval_pearsonr': 0.9620726741706105, 'eval_runtime': 4.5389, 'eval_samples_per_second': 257.113, 'eval_steps_per_second': 4.186, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:39<03:08,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0278, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:58<01:52,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2369728833436966, 'eval_pearsonr': 0.9609151284168795, 'eval_runtime': 4.5749, 'eval_samples_per_second': 255.086, 'eval_steps_per_second': 4.153, 'epoch': 9.09}\n",
            " 91% 1500/1650 [21:03<01:52,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.0246, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:24<00:37,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.12it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.70it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.26458123326301575, 'eval_pearsonr': 0.9610700503500563, 'eval_runtime': 4.6998, 'eval_samples_per_second': 248.309, 'eval_steps_per_second': 4.043, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:28<00:37,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [23:11<00:00,  1.79it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1300 (score: 0.963302000959181).\n",
            "{'train_runtime': 1392.9673, 'train_samples_per_second': 75.386, 'train_steps_per_second': 1.185, 'train_loss': 0.14045556184017297, 'epoch': 10.0}\n",
            "100% 1650/1650 [23:12<00:00,  1.18it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/sts/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "ntXb8iLDnhFh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>2.85).astype(int)\n",
        "print(\"epochs 10 / >2.85\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUl3KNS8nm4b",
        "outputId": "c4af0d14-52b5-431d-fd2d-2a04e95d5598"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.71      0.83       299\n",
            "           1       0.71      0.99      0.83       220\n",
            "\n",
            "    accuracy                           0.83       519\n",
            "   macro avg       0.85      0.85      0.83       519\n",
            "weighted avg       0.87      0.83      0.83       519\n",
            "\n",
            "custom_model(roberta-base) f1_score: 0.8304761904761905\n",
            "\n",
            "epochs 10 / >2.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.80      0.88       299\n",
            "           1       0.78      0.97      0.86       220\n",
            "\n",
            "    accuracy                           0.87       519\n",
            "   macro avg       0.88      0.88      0.87       519\n",
            "weighted avg       0.89      0.87      0.87       519\n",
            "\n",
            "custom_model(roberta-base) f1_score: 0.8640973630831643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY7js93vrggG",
        "outputId": "18dd2840-d5c4-489c-f855-4d512c940bc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8981787692339827}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 데이터의 전처리에 따른 성능 비교\n",
        "\n",
        "전처리 1 : 영어, 숫자, 특수문자 전처리\n",
        "\n",
        "전처리 2 : 영어만 소문자 처리\n",
        "\n",
        "다음의 두가지 방식으로 전처리 진행후 성능을 비교하였습니다.\n",
        "\n",
        "결과 전처리를 통한 성능 감소를 확인 하였고 전처리를 진행하지 않고 학습시킨 custom 모델을 최종 선정하였습니다."
      ],
      "metadata": {
        "id": "_fvk2foj5IXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# 파일 업로드\n",
        "\n",
        "with open('/content/klue-sts-v1.1/klue-sts-v1.1_train.json', 'r') as f:\n",
        "\n",
        "  json_data_1 = json.load(f)\n",
        "  json_data_2 = copy.deepcopy(json_data_1)\n",
        "\n",
        "  json.dumps(json_data_1, ensure_ascii=False)\n",
        "  json.dumps(json_data_2, ensure_ascii=False)\n",
        "\n",
        "with open('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json', 'r') as f:\n",
        "\n",
        "  json_data_dev_1 = json.load(f)\n",
        "  json_data_dev_2 = copy.deepcopy(json_data_dev_1)\n",
        "\n",
        "  json.dumps(json_data_dev_1, ensure_ascii=False)\n",
        "  json.dumps(json_data_dev_2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "1ImjjUa55HW_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 전처리 두가지 1. 영어, 숫자, 특수문자 전처리 2. 영어만 소문자 처리\n",
        "\n",
        "# 1번 전처리(영어, 숫자, 특수문자 전처리)\n",
        "def delete_1(json_data):\n",
        "  for idx in range(len(json_data)):\n",
        "    json_data[idx]['sentence1'] = re.sub('[0-9a-zA-Z-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘〈〉|\\(\\)\\[\\]\\<\\>`\\'…》《]','', json_data[idx]['sentence1'])\n",
        "    json_data[idx]['sentence2'] = re.sub('[0-9a-zA-Z-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘〈〉|\\(\\)\\[\\]\\<\\>`\\'…》《]','', json_data[idx]['sentence2'])\n",
        "  return json_data\n",
        "\n",
        "# 2번 전처리(영어만 소문자 처리)\n",
        "def delete_2(json_data):\n",
        "  for idx in range(len(json_data)):\n",
        "    json_data[idx]['sentence1'] = json_data[idx]['sentence1'].lower()\n",
        "    json_data[idx]['sentence2'] = json_data[idx]['sentence2'].lower()\n",
        "  return json_data\n"
      ],
      "metadata": {
        "id": "TdKP1Yjo5vb9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리 진행 \n",
        "\n",
        "train_1 = delete_1(json_data_1)\n",
        "train_2 = delete_2(json_data_2)\n",
        "dev_1 = delete_1(json_data_dev_1)\n",
        "dev_2 = delete_1(json_data_dev_2)\n",
        "\n",
        "# 전처리 결과 확인\n",
        "\n",
        "print(\"전처리 1 :\",train_1[7162]['sentence1'])\n",
        "print(\"전처리 2 :\",train_2[7162]['sentence1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3pcRjJc5wr9",
        "outputId": "fb74af2e-5227-43b6-f568-8585aee300a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 1 : 올해 예산 이·전용분 억 원 중 억 원은 코박스 퍼실러티  가입을 위한 선급금으로 이미 집행했다\n",
            "전처리 2 : 올해 예산 이·전용분 1723억 원 중 850억 원은 코박스 퍼실러티(covax facility) 가입을 위한 선급금으로 이미 집행했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 파일 json파일 저장\n",
        "\n",
        "with open('/content/train_1.json','w') as f:\n",
        "  json.dump(train_1, f, ensure_ascii=False)\n",
        "\n",
        "with open('/content/train_2.json','w') as f:\n",
        "  json.dump(train_2, f, ensure_ascii=False)\n",
        "\n",
        "with open('/content/dev_1.json','w') as f:\n",
        "  json.dump(dev_1, f, ensure_ascii=False)\n",
        "\n",
        "with open('/content/dev_2.json','w') as f:\n",
        "  json.dump(dev_2, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "aaaMnaKC52w2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "for num in range(1,3):\n",
        "  data=read_json(f'/content/train_{num}.json')\n",
        "\n",
        "  train_length=int(len(data)*0.9)\n",
        "  train=data[:train_length]\n",
        "  vaild=data[train_length:]\n",
        "\n",
        "  print('data_set:',len(data))\n",
        "  print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "  with open(f'train_split{num}.json','w') as f:\n",
        "    json.dump(train,f,ensure_ascii = False)\n",
        "  with open(f'valid_split{num}.json','w') as f:\n",
        "    json.dump(vaild,f,ensure_ascii = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4YUAThe57bO",
        "outputId": "bbee3d41-1d8f-4051-bce2-51a542a557e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n",
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리 1에 따른 모델 학습 및 성능 비교"
      ],
      "metadata": {
        "id": "HNCqkgfP8c0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번 전처리 데이터로 custom 모델 학습\n",
        "!python /content/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split1.json\" --valid_filename \"valid_split1.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS_2ZxvB6mu6",
        "outputId": "c8b321db-4d84-440d-911b-cc68364059ac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'cls_fc_layer.linear.bias', 'dense2.linear.weight', 'cls_fc_layer.linear.weight', 'label_classifier.linear.weight', 'sentence_fc_layer3.linear.bias', 'dense.linear.weight', 'sentence_fc_layer2.linear.weight', 'sentence_fc_layer.linear.weight', 'sentence_fc_layer.linear.bias', 'label_classifier.linear.bias', 'sentence_fc_layer3.linear.weight', 'embedding_vectors.linear.bias', 'dense2.linear.bias', 'dense.linear.bias', 'sentence_fc_layer2.linear.bias', 'embedding_vectors.linear.weight', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.204, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:06<16:30,  1.56it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.18it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.49it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.22it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.00it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.86it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.77it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.67it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.61it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.61it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.60it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.59it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.45757952332496643, 'eval_pearsonr': 0.9424598666291135, 'eval_runtime': 4.4795, 'eval_samples_per_second': 260.522, 'eval_steps_per_second': 4.242, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:11<16:30,  1.56it/s]\n",
            "100% 19/19 [00:04<00:00,  4.59it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.3132, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:20<15:21,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.23it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.87it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.78it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.69it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3767984211444855, 'eval_pearsonr': 0.948395262572398, 'eval_runtime': 4.3128, 'eval_samples_per_second': 270.59, 'eval_steps_per_second': 4.405, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:24<15:21,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.2268, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [03:33<14:25,  1.56it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.25it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.51it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.23415517807006836, 'eval_pearsonr': 0.9545395047680885, 'eval_runtime': 4.3072, 'eval_samples_per_second': 270.942, 'eval_steps_per_second': 4.411, 'epoch': 1.82}\n",
            " 18% 300/1650 [03:37<14:25,  1.56it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1889, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [04:45<13:15,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.25it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.61it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.22it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.87it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2712801396846771, 'eval_pearsonr': 0.9549562639485479, 'eval_runtime': 4.2969, 'eval_samples_per_second': 271.591, 'eval_steps_per_second': 4.422, 'epoch': 2.42}\n",
            " 24% 400/1650 [04:50<13:15,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1524, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [05:58<11:40,  1.64it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.14it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.49it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.64it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.66it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.41990694403648376, 'eval_pearsonr': 0.951841167449826, 'eval_runtime': 4.3018, 'eval_samples_per_second': 271.283, 'eval_steps_per_second': 4.417, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:02<11:40,  1.64it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.113, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [07:11<11:06,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.22it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.66it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2675786316394806, 'eval_pearsonr': 0.9585646547444525, 'eval_runtime': 4.4905, 'eval_samples_per_second': 259.881, 'eval_steps_per_second': 4.231, 'epoch': 3.64}\n",
            " 36% 600/1650 [07:15<11:06,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.61it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.1027, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [08:24<10:05,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.20it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.51it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.86it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.78it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.69it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2547280490398407, 'eval_pearsonr': 0.9567651469285702, 'eval_runtime': 4.3262, 'eval_samples_per_second': 269.749, 'eval_steps_per_second': 4.392, 'epoch': 4.24}\n",
            " 42% 700/1650 [08:28<10:05,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0895, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [09:37<09:00,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.25it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.60it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.21it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.00it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.86it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.78it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.68it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.66it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.61it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.61it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2964992821216583, 'eval_pearsonr': 0.9545829884161412, 'eval_runtime': 4.3068, 'eval_samples_per_second': 270.964, 'eval_steps_per_second': 4.412, 'epoch': 4.85}\n",
            " 48% 800/1650 [09:41<09:00,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.61it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0751, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [10:50<07:57,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.13it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.48it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.23it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3493235111236572, 'eval_pearsonr': 0.9537426196305426, 'eval_runtime': 4.2947, 'eval_samples_per_second': 271.731, 'eval_steps_per_second': 4.424, 'epoch': 5.45}\n",
            " 55% 900/1650 [10:54<07:57,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0651, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [12:02<06:49,  1.59it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.23it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.51it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.23it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.33343401551246643, 'eval_pearsonr': 0.9567191539629503, 'eval_runtime': 4.3042, 'eval_samples_per_second': 271.132, 'eval_steps_per_second': 4.414, 'epoch': 6.06}\n",
            " 61% 1000/1650 [12:07<06:49,  1.59it/s]\n",
            "100% 19/19 [00:04<00:00,  4.61it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0538, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [13:16<05:50,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.25it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.21it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.97it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.85it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.78it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.20720922946929932, 'eval_pearsonr': 0.9595210266168703, 'eval_runtime': 4.4622, 'eval_samples_per_second': 261.528, 'eval_steps_per_second': 4.258, 'epoch': 6.67}\n",
            " 67% 1100/1650 [13:20<05:50,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0517, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [14:28<04:45,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.08it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.48it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.22it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.66it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2405877262353897, 'eval_pearsonr': 0.9586346995178876, 'eval_runtime': 4.3125, 'eval_samples_per_second': 270.611, 'eval_steps_per_second': 4.406, 'epoch': 7.27}\n",
            " 73% 1200/1650 [14:33<04:45,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.0447, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [15:41<03:43,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.23it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.48it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.23it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.68it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.66it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24021068215370178, 'eval_pearsonr': 0.9568488298998177, 'eval_runtime': 4.3029, 'eval_samples_per_second': 271.212, 'eval_steps_per_second': 4.416, 'epoch': 7.88}\n",
            " 79% 1300/1650 [15:46<03:43,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0381, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [16:54<02:39,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.24it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.52it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.65it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.24it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  5.01it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.88it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.79it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.70it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.67it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.63it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2937571108341217, 'eval_pearsonr': 0.9565902044545221, 'eval_runtime': 4.3106, 'eval_samples_per_second': 270.729, 'eval_steps_per_second': 4.408, 'epoch': 8.48}\n",
            " 85% 1400/1650 [16:59<02:39,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.62it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.0364, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [18:07<01:35,  1.57it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.21it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.50it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.63it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.22it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.98it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.85it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.77it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.65it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.63it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.61it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.61it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.60it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.60it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.60it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.60it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2291794866323471, 'eval_pearsonr': 0.9585334195969869, 'eval_runtime': 4.3467, 'eval_samples_per_second': 268.48, 'eval_steps_per_second': 4.371, 'epoch': 9.09}\n",
            " 91% 1500/1650 [18:11<01:35,  1.57it/s]\n",
            "100% 19/19 [00:04<00:00,  4.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.0326, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [19:20<00:31,  1.56it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  9.19it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.42it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.59it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  5.20it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.99it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.85it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.76it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 53% 10/19 [00:01<00:01,  4.67it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.65it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.64it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.62it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.61it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.61it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.60it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2659783363342285, 'eval_pearsonr': 0.9583793736529073, 'eval_runtime': 4.4686, 'eval_samples_per_second': 261.154, 'eval_steps_per_second': 4.252, 'epoch': 9.7}\n",
            " 97% 1600/1650 [19:25<00:31,  1.56it/s]\n",
            "100% 19/19 [00:04<00:00,  4.60it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [20:01<00:00,  2.09it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1100 (score: 0.9595210266168703).\n",
            "{'train_runtime': 1203.8095, 'train_samples_per_second': 87.231, 'train_steps_per_second': 1.371, 'train_loss': 0.169943111520825, 'epoch': 10.0}\n",
            "100% 1650/1650 [20:03<00:00,  1.37it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/sts/inference.py --test_filename \"/content/dev_1.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "deImcjyq64Pg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/dev_1.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base / 전처리1) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>3.2).astype(int)\n",
        "print(\"epochs 10 / >3.2\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base / 전처리1) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9BGcC-48oV3",
        "outputId": "37bc5544-649f-49ab-bac9-a3b43c534e9d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.66      0.79       299\n",
            "           1       0.68      0.98      0.80       220\n",
            "\n",
            "    accuracy                           0.80       519\n",
            "   macro avg       0.83      0.82      0.80       519\n",
            "weighted avg       0.85      0.80      0.80       519\n",
            "\n",
            "custom_model(roberta-base / 전처리1) f1_score: 0.8044692737430168\n",
            "\n",
            "epochs 10 / >3.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.82      0.86       299\n",
            "           1       0.78      0.90      0.83       220\n",
            "\n",
            "    accuracy                           0.85       519\n",
            "   macro avg       0.85      0.86      0.85       519\n",
            "weighted avg       0.86      0.85      0.85       519\n",
            "\n",
            "custom_model(roberta-base / 전처리1) f1_score: 0.8347457627118645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/dev_1.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdDHsCBd8tOX",
        "outputId": "eeb8c817-a5be-4e20-cb8a-dd81ac15484a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8468598368217276}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리 2에 따른 모델 학습 및 성능 비교"
      ],
      "metadata": {
        "id": "HKr9n4dm8hjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2번 전처리 데이터로 custom 모델 학습\n",
        "!python /content/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split2.json\" --valid_filename \"valid_split2.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPCczLd06w9h",
        "outputId": "441a50af-338e-4c66-ae1f-b49e1f442f90"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'sentence_fc_layer3.linear.weight', 'dense2.linear.bias', 'cls_fc_layer.linear.weight', 'dense.linear.weight', 'embedding_vectors.linear.bias', 'sentence_fc_layer.linear.bias', 'roberta.pooler.dense.bias', 'cls_fc_layer.linear.bias', 'sentence_fc_layer.linear.weight', 'dense.linear.bias', 'sentence_fc_layer2.linear.bias', 'label_classifier.linear.weight', 'sentence_fc_layer3.linear.bias', 'sentence_fc_layer2.linear.weight', 'embedding_vectors.linear.weight', 'dense2.linear.weight', 'label_classifier.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.0966, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:16<19:27,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.69it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.34760144352912903, 'eval_pearsonr': 0.947007304215065, 'eval_runtime': 4.5654, 'eval_samples_per_second': 255.62, 'eval_steps_per_second': 4.162, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:20<19:27,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.2241, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:40<18:10,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.41723787784576416, 'eval_pearsonr': 0.9529568019822883, 'eval_runtime': 4.5702, 'eval_samples_per_second': 255.353, 'eval_steps_per_second': 4.157, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:45<18:10,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.178, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:06<16:56,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.12it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.366951584815979, 'eval_pearsonr': 0.9553900796964083, 'eval_runtime': 4.691, 'eval_samples_per_second': 248.774, 'eval_steps_per_second': 4.05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:11<16:56,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1437, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:31<15:44,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.19539567828178406, 'eval_pearsonr': 0.9596661136448411, 'eval_runtime': 4.5694, 'eval_samples_per_second': 255.393, 'eval_steps_per_second': 4.158, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:35<15:44,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1201, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:55<13:49,  1.39it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.60it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.11it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.44it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.34078630805015564, 'eval_pearsonr': 0.9568215147771879, 'eval_runtime': 4.5725, 'eval_samples_per_second': 255.22, 'eval_steps_per_second': 4.155, 'epoch': 3.03}\n",
            " 30% 500/1650 [07:00<13:49,  1.39it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.0908, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:20<13:10,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.64it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.11it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.92it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.68it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.55it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.44it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.40it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.33it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.32748979330062866, 'eval_pearsonr': 0.958084570360059, 'eval_runtime': 4.5679, 'eval_samples_per_second': 255.476, 'eval_steps_per_second': 4.159, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:25<13:10,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.33it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0869, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:45<11:58,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.54it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.04it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.27it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.90it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.70it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.57it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.42it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.39it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3542558252811432, 'eval_pearsonr': 0.9586986499941942, 'eval_runtime': 4.7206, 'eval_samples_per_second': 247.213, 'eval_steps_per_second': 4.025, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:50<11:58,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0723, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:10<10:39,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.34it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.24737109243869781, 'eval_pearsonr': 0.9619206519308382, 'eval_runtime': 4.5701, 'eval_samples_per_second': 255.358, 'eval_steps_per_second': 4.158, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:15<10:39,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.0605, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:35<09:24,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.29it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.92it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2674124836921692, 'eval_pearsonr': 0.9591184676027603, 'eval_runtime': 4.5596, 'eval_samples_per_second': 255.946, 'eval_steps_per_second': 4.167, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:39<09:24,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0544, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:59<08:07,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.10it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.35it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23565775156021118, 'eval_pearsonr': 0.9607373867872868, 'eval_runtime': 4.5648, 'eval_samples_per_second': 255.65, 'eval_steps_per_second': 4.162, 'epoch': 6.06}\n",
            " 61% 1000/1650 [14:04<08:07,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0453, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:25<06:55,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.41it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.1931891143321991, 'eval_pearsonr': 0.9636452500798474, 'eval_runtime': 4.6818, 'eval_samples_per_second': 249.266, 'eval_steps_per_second': 4.058, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:29<06:55,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0421, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:49<05:38,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.29it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.92it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.71it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2454395890235901, 'eval_pearsonr': 0.9618986172957115, 'eval_runtime': 4.5602, 'eval_samples_per_second': 255.909, 'eval_steps_per_second': 4.166, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:54<05:38,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.0374, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:14<04:23,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.13it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.29it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.92it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24012121558189392, 'eval_pearsonr': 0.9625251155609411, 'eval_runtime': 4.5465, 'eval_samples_per_second': 256.682, 'eval_steps_per_second': 4.179, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:19<04:23,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.033, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:39<03:08,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.60it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.12it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.31it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.93it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2268693447113037, 'eval_pearsonr': 0.9631499631334332, 'eval_runtime': 4.5582, 'eval_samples_per_second': 256.022, 'eval_steps_per_second': 4.168, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:44<03:08,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.33it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.0293, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [21:04<01:52,  1.33it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.67it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.09it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.30it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.92it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.72it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.51it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24619314074516296, 'eval_pearsonr': 0.9629003320475176, 'eval_runtime': 4.6993, 'eval_samples_per_second': 248.334, 'eval_steps_per_second': 4.043, 'epoch': 9.09}\n",
            " 91% 1500/1650 [21:08<01:52,  1.33it/s]\n",
            "100% 19/19 [00:04<00:00,  4.34it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.0252, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:29<00:37,  1.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.58it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.45it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.42it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.35it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.34it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2482665628194809, 'eval_pearsonr': 0.9625695954084118, 'eval_runtime': 4.5499, 'eval_samples_per_second': 256.491, 'eval_steps_per_second': 4.176, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:33<00:37,  1.32it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [23:16<00:00,  1.78it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1100 (score: 0.9636452500798474).\n",
            "{'train_runtime': 1398.133, 'train_samples_per_second': 75.107, 'train_steps_per_second': 1.18, 'train_loss': 0.14258333726362749, 'epoch': 10.0}\n",
            "100% 1650/1650 [23:17<00:00,  1.18it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/sts/inference.py --test_filename \"/content/dev_2.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "jho4lBrv64p0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/dev_2.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base / 전처리2) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>2.8).astype(int)\n",
        "print(\"epochs 10 / >2.8\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base / 전처리2) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQzSET4y8o2D",
        "outputId": "6701d52f-cde4-41fa-f34c-99ba50f7957c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.70      0.81       299\n",
            "           1       0.70      0.96      0.81       220\n",
            "\n",
            "    accuracy                           0.81       519\n",
            "   macro avg       0.83      0.83      0.81       519\n",
            "weighted avg       0.85      0.81      0.81       519\n",
            "\n",
            "custom_model(roberta-base / 전처리2) f1_score: 0.8084291187739464\n",
            "\n",
            "epochs 10 / >2.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.77      0.84       299\n",
            "           1       0.75      0.92      0.82       220\n",
            "\n",
            "    accuracy                           0.83       519\n",
            "   macro avg       0.84      0.84      0.83       519\n",
            "weighted avg       0.85      0.83      0.83       519\n",
            "\n",
            "custom_model(roberta-base / 전처리2) f1_score: 0.8228105906313646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/dev_2.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpSUoecM8t7v",
        "outputId": "e837b9ca-4a07-4f65-dd7f-a1194deaf724"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8503741504839647}\n"
          ]
        }
      ]
    }
  ]
}