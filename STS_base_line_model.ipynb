{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS_base_line_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# base_line 모델(roberta-base)의 선정 및 f1_score \n",
        "\n",
        "Team LostCow의 roberta모델을 기준으로 함\n",
        "\n",
        "출처: https://github.com/l-yohai/KLUE"
      ],
      "metadata": {
        "id": "pmsQRpsmRRfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez-KaYpLFd04",
        "outputId": "e062e5ad-f03a-4738-93e8-cebf1fb9037a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 46.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 10.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "#훈련 데이터셋 다운로드 및 압축풀기\n",
        "\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
        "\n",
        "tar_bz2_file = tarfile.open(\"/content/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=\"/content\")\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "3J5pSUgIHv0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736c03c8-c9aa-4fe4-852f-93d570b66359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 00:19:46--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.218.233.203\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.218.233.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-03-17 00:19:46 (14.4 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#GPU 사용 여부 확인 및 name 확인\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "oZoPB1WKJnet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a528e5b9-bab5-4101-9bb6-499aff9b8461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jy5VuDP_w1T",
        "outputId": "b502f74b-65c2-48f9-edec-a26741b77cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "\n",
        "train_length=int(len(data)*0.9)\n",
        "train=data[:train_length]\n",
        "vaild=data[train_length:]\n",
        "\n",
        "print('data_set:',len(data))\n",
        "print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "with open('train_split.json','w') as f:\n",
        "  json.dump(train,f,ensure_ascii = False)\n",
        "with open('valid_split.json','w') as f:\n",
        "  json.dump(vaild,f,ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/l-yohai/KLUE #base line 모델 불러오기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7hnyZVgAuf2",
        "outputId": "1bf2b578-e15e-4eb0-c9bf-934f753a9d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KLUE'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 313 (delta 52), reused 101 (delta 29), pack-reused 179\u001b[K\n",
            "Receiving objects: 100% (313/313), 5.79 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# base line 모델 학습\n",
        "!python /content/KLUE/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvAb8SjFdMR",
        "outputId": "48012329-1d4e-474e-cd6b-0b07f3b11a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['cls_fc_layer.linear.weight', 'sentence_fc_layer.linear.bias', 'dense.linear.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'label_classifier.linear.weight', 'label_classifier.linear.bias', 'sentence_fc_layer.linear.weight', 'cls_fc_layer.linear.bias', 'dense.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.1313, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:14<19:13,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.77it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.76it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.22365519404411316, 'eval_pearsonr': 0.9507152306522215, 'eval_runtime': 4.5377, 'eval_samples_per_second': 257.181, 'eval_steps_per_second': 4.187, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:18<19:13,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.2105, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:37<17:57,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5125826001167297, 'eval_pearsonr': 0.946132521751175, 'eval_runtime': 4.6456, 'eval_samples_per_second': 251.204, 'eval_steps_per_second': 4.09, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:41<17:57,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.1673, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:01<16:46,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.16780120134353638, 'eval_pearsonr': 0.9623197233976968, 'eval_runtime': 4.558, 'eval_samples_per_second': 256.031, 'eval_steps_per_second': 4.168, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:05<16:46,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1224, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:24<15:28,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2284928411245346, 'eval_pearsonr': 0.9581630776284613, 'eval_runtime': 4.5058, 'eval_samples_per_second': 259.001, 'eval_steps_per_second': 4.217, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:28<15:28,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1131, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:47<13:36,  1.41it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2404332458972931, 'eval_pearsonr': 0.95887289742035, 'eval_runtime': 4.5079, 'eval_samples_per_second': 258.88, 'eval_steps_per_second': 4.215, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:52<13:36,  1.41it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.0858, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:11<12:59,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.26666244864463806, 'eval_pearsonr': 0.9607266825802501, 'eval_runtime': 4.7416, 'eval_samples_per_second': 246.118, 'eval_steps_per_second': 4.007, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:16<12:59,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0767, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:34<11:45,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.26778799295425415, 'eval_pearsonr': 0.960294526518308, 'eval_runtime': 4.5198, 'eval_samples_per_second': 258.2, 'eval_steps_per_second': 4.204, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:39<11:45,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0596, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [10:58<10:31,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.55it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.22287163138389587, 'eval_pearsonr': 0.9585569179329159, 'eval_runtime': 4.5415, 'eval_samples_per_second': 256.963, 'eval_steps_per_second': 4.184, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:03<10:31,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.054, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:21<09:16,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2512325346469879, 'eval_pearsonr': 0.9604608687973275, 'eval_runtime': 4.5231, 'eval_samples_per_second': 258.008, 'eval_steps_per_second': 4.201, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:26<09:16,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0491, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:44<07:59,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.46it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2536238729953766, 'eval_pearsonr': 0.9605980182317052, 'eval_runtime': 4.6591, 'eval_samples_per_second': 250.478, 'eval_steps_per_second': 4.078, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:49<07:59,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0366, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:08<06:49,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.18993763625621796, 'eval_pearsonr': 0.9603322574005452, 'eval_runtime': 4.5355, 'eval_samples_per_second': 257.301, 'eval_steps_per_second': 4.189, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:13<06:49,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.035, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:32<05:34,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.63it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.251998633146286, 'eval_pearsonr': 0.9609871499442095, 'eval_runtime': 4.523, 'eval_samples_per_second': 258.012, 'eval_steps_per_second': 4.201, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:36<05:34,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 0.0306, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [17:55<04:19,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2188197672367096, 'eval_pearsonr': 0.9598166926080405, 'eval_runtime': 4.5463, 'eval_samples_per_second': 256.691, 'eval_steps_per_second': 4.179, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:00<04:19,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0267, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:19<03:05,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.18it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.36it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.97it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.39it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21170051395893097, 'eval_pearsonr': 0.9609987739515057, 'eval_runtime': 4.6494, 'eval_samples_per_second': 250.997, 'eval_steps_per_second': 4.087, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:23<03:05,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.0242, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:42<01:51,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21484673023223877, 'eval_pearsonr': 0.9624502743855923, 'eval_runtime': 4.5195, 'eval_samples_per_second': 258.212, 'eval_steps_per_second': 4.204, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:46<01:51,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.0216, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:06<00:37,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.49it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.43it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23112452030181885, 'eval_pearsonr': 0.9630249943800897, 'eval_runtime': 4.5336, 'eval_samples_per_second': 257.41, 'eval_steps_per_second': 4.191, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:10<00:37,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "100% 1650/1650 [22:51<00:00,  1.81it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1600 (score: 0.9630249943800897).\n",
            "{'train_runtime': 1372.3942, 'train_samples_per_second': 76.516, 'train_steps_per_second': 1.202, 'train_loss': 0.1366526355165424, 'epoch': 10.0}\n",
            "100% 1650/1650 [22:52<00:00,  1.20it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/klue-sts-v1.1.tar.gz /content/model/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "id": "8tWlmn5hONdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 base line 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/KLUE/sts/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "P_WMvlynKy3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]"
      ],
      "metadata": {
        "id": "94-yxLA9Ny2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pandas as pd\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, df_pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, df_pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>3.1).astype(int)\n",
        "print(\"epochs 10 / >3.1\")\n",
        "print(classification_report(valid_label, df_pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, df_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsHBuMmMUYRb",
        "outputId": "a60a662c-f453-4802-f78b-c0da075b5b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.66      0.79       299\n",
            "           1       0.68      1.00      0.81       220\n",
            "\n",
            "    accuracy                           0.80       519\n",
            "   macro avg       0.84      0.83      0.80       519\n",
            "weighted avg       0.86      0.80      0.80       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8103130755064457\n",
            "\n",
            "epochs 10 / >3.1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.80      0.87       299\n",
            "           1       0.78      0.96      0.86       220\n",
            "\n",
            "    accuracy                           0.87       519\n",
            "   macro avg       0.87      0.88      0.87       519\n",
            "weighted avg       0.89      0.87      0.87       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.859470468431772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnSRP83NiuA2",
        "outputId": "ff97efc1-8594-4f48-c29e-a445ca92f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8939478658048167}\n"
          ]
        }
      ]
    }
  ]
}