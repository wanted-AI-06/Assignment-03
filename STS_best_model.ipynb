{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS_custom_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모델(roberta-base)의 custom 및 f1_score & pearsonr \n",
        "\n",
        "Team LostCow의 roberta모델을 기준으로 함\n",
        "\n",
        "출처: https://github.com/l-yohai/KLUE"
      ],
      "metadata": {
        "id": "pmsQRpsmRRfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez-KaYpLFd04",
        "outputId": "2c839b79-ec62-4cfe-be16-03d699c4f8ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 16.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "#훈련 데이터셋 다운로드 및 압축풀기\n",
        "\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
        "\n",
        "tar_bz2_file = tarfile.open(\"/content/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=\"/content\")\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "3J5pSUgIHv0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e382aba0-7fcf-4404-9d40-7cdfe7b4d716"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-18 00:09:49--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.218.178.177\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.218.178.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-03-18 00:09:49 (14.9 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#GPU 사용 여부 확인 및 name 확인\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "oZoPB1WKJnet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003b69d5-753c-4b44-ad66-0a058fed59c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jy5VuDP_w1T",
        "outputId": "9867389c-c66f-48ec-fc3f-cfc1432f8c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "\n",
        "train_length=int(len(data)*0.9)\n",
        "train=data[:train_length]\n",
        "vaild=data[train_length:]\n",
        "\n",
        "print('data_set:',len(data))\n",
        "print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "with open('train_split.json','w') as f:\n",
        "  json.dump(train,f,ensure_ascii = False)\n",
        "with open('valid_split.json','w') as f:\n",
        "  json.dump(vaild,f,ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 custom을 진행한 폴더 압축해제\n",
        "!unzip /content/KLUE_custom.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7hnyZVgAuf2",
        "outputId": "0204842b-835f-46b4-dc43-e60b27fc49c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/KLUE_custom.zip\n",
            "  inflating: sts/__pycache__/dataset.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/metric.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/model.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/utils.cpython-37.pyc  \n",
            "  inflating: sts/data/download.sh    \n",
            "  inflating: sts/README.md           \n",
            "  inflating: sts/dataloader.py       \n",
            "  inflating: sts/dataset.py          \n",
            "  inflating: sts/infer_test.ipynb    \n",
            "  inflating: sts/inference.py        \n",
            "  inflating: sts/metric.py           \n",
            "  inflating: sts/model.py            \n",
            " extracting: sts/requirements.txt    \n",
            "  inflating: sts/train.py            \n",
            "  inflating: sts/utils.py            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom 모델 학습\n",
        "!python /content/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvAb8SjFdMR",
        "outputId": "712f23c1-68d9-4eed-dc9b-e33473245cc9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['cls_fc_layer.linear.bias', 'sentence_fc_layer3.linear.weight', 'dense2.linear.bias', 'dense2.linear.weight', 'roberta.pooler.dense.bias', 'sentence_fc_layer.linear.bias', 'roberta.pooler.dense.weight', 'dense.linear.weight', 'sentence_fc_layer.linear.weight', 'sentence_fc_layer2.linear.weight', 'label_classifier.linear.bias', 'sentence_fc_layer2.linear.bias', 'cls_fc_layer.linear.weight', 'sentence_fc_layer3.linear.bias', 'dense.linear.bias', 'embedding_vectors.linear.bias', 'embedding_vectors.linear.weight', 'label_classifier.linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.292, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:14<19:16,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6280980706214905, 'eval_pearsonr': 0.945154977297672, 'eval_runtime': 4.7244, 'eval_samples_per_second': 247.016, 'eval_steps_per_second': 4.022, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:19<19:16,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.2768, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:38<18:01,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6456100344657898, 'eval_pearsonr': 0.951510022652817, 'eval_runtime': 4.5222, 'eval_samples_per_second': 258.062, 'eval_steps_per_second': 4.202, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:42<18:01,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.2124, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:02<16:46,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7550191879272461, 'eval_pearsonr': 0.9596681128526559, 'eval_runtime': 4.5185, 'eval_samples_per_second': 258.273, 'eval_steps_per_second': 4.205, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:07<16:46,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1696, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:26<15:33,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8967095017433167, 'eval_pearsonr': 0.9581463466453473, 'eval_runtime': 4.5545, 'eval_samples_per_second': 256.227, 'eval_steps_per_second': 4.172, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:30<15:33,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1573, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:49<13:41,  1.40it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7018004059791565, 'eval_pearsonr': 0.9585858230420801, 'eval_runtime': 4.597, 'eval_samples_per_second': 253.86, 'eval_steps_per_second': 4.133, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:54<13:41,  1.40it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.1256, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:14<13:03,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.59it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8007280230522156, 'eval_pearsonr': 0.9600028186330775, 'eval_runtime': 4.5212, 'eval_samples_per_second': 258.115, 'eval_steps_per_second': 4.202, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:18<13:03,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.1177, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:37<11:49,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5089148283004761, 'eval_pearsonr': 0.9590802880643732, 'eval_runtime': 4.5155, 'eval_samples_per_second': 258.444, 'eval_steps_per_second': 4.208, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:42<11:49,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.1015, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:02<10:34,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.45it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6314062476158142, 'eval_pearsonr': 0.9616287678453487, 'eval_runtime': 4.518, 'eval_samples_per_second': 258.299, 'eval_steps_per_second': 4.205, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:06<10:34,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0964, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:25<09:19,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.75it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.54it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6356558799743652, 'eval_pearsonr': 0.9623446278856889, 'eval_runtime': 4.665, 'eval_samples_per_second': 250.158, 'eval_steps_per_second': 4.073, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:30<09:19,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0822, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:49<08:01,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.17it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.35it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.578285813331604, 'eval_pearsonr': 0.960739909186852, 'eval_runtime': 4.532, 'eval_samples_per_second': 257.501, 'eval_steps_per_second': 4.192, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:53<08:01,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.38it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0745, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:13<06:51,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.5152141451835632, 'eval_pearsonr': 0.963133159769037, 'eval_runtime': 4.519, 'eval_samples_per_second': 258.244, 'eval_steps_per_second': 4.204, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:18<06:51,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.07, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:37<05:35,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7151077389717102, 'eval_pearsonr': 0.9628407040192676, 'eval_runtime': 4.5519, 'eval_samples_per_second': 256.374, 'eval_steps_per_second': 4.174, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:41<05:35,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.0644, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:01<04:21,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.460258424282074, 'eval_pearsonr': 0.9625171669448565, 'eval_runtime': 4.6548, 'eval_samples_per_second': 250.708, 'eval_steps_per_second': 4.082, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:06<04:21,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0599, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:25<03:06,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6882723569869995, 'eval_pearsonr': 0.9627319542913869, 'eval_runtime': 4.5279, 'eval_samples_per_second': 257.734, 'eval_steps_per_second': 4.196, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:29<03:06,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 0.0532, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:49<01:51,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.32it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6567316651344299, 'eval_pearsonr': 0.9630658179567396, 'eval_runtime': 4.5259, 'eval_samples_per_second': 257.851, 'eval_steps_per_second': 4.198, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:53<01:51,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.05, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:13<00:37,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.7082459330558777, 'eval_pearsonr': 0.9629077726678906, 'eval_runtime': 4.5641, 'eval_samples_per_second': 255.69, 'eval_steps_per_second': 4.163, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:18<00:37,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [22:59<00:00,  1.80it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1100 (score: 0.963133159769037).\n",
            "{'train_runtime': 1382.009, 'train_samples_per_second': 75.984, 'train_steps_per_second': 1.194, 'train_loss': 0.18349551403161252, 'epoch': 10.0}\n",
            "100% 1650/1650 [23:01<00:00,  1.19it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/sts/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "P_WMvlynKy3S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pandas as pd\n",
        "\n",
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>3.2).astype(int)\n",
        "print(\"epochs 10 / >3.2\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "id": "lof2YeveXdBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76a6aee-6cd1-49b0-9163-3c0922cc2cb1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.69      0.81       299\n",
            "           1       0.70      1.00      0.82       220\n",
            "\n",
            "    accuracy                           0.82       519\n",
            "   macro avg       0.85      0.84      0.82       519\n",
            "weighted avg       0.87      0.82      0.82       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8239700374531835\n",
            "\n",
            "epochs 10 / >3.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.81      0.88       299\n",
            "           1       0.79      0.96      0.87       220\n",
            "\n",
            "    accuracy                           0.88       519\n",
            "   macro avg       0.88      0.89      0.88       519\n",
            "weighted avg       0.89      0.88      0.88       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8688524590163933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZOYVIlAsqWd",
        "outputId": "82ef41e4-6885-4a7b-f7aa-020d0278d8c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.9020511636828425}\n"
          ]
        }
      ]
    }
  ]
}