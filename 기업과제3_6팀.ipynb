{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_03_STS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9cL24awwZ83",
        "outputId": "596e39f4-98c5-4184-be63-d38a12cf78fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 43.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 41.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 63.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 134 kB 40.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 49.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets wandb -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team [LostCow](https://github.com/l-yohai/KLUE)의 roberta모델을 기준으로 함\n",
        "- pretrained_model : klue/roberta-base\n",
        "- tokenizer : klue/roberta-base\n",
        "- dropout : 0.2\n",
        "- batch_size : 64\n",
        "- optim : adamw_hf\n",
        "- learning_rate : 5e-5\n",
        "- loss_fnc : MSELoss"
      ],
      "metadata": {
        "id": "FrcDvF1bxgHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tarfile\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report,f1_score\n",
        "\n",
        "import torch\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "rwKPDKiox4OD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 사용 여부 확인 및 name 확인\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4S4jNGEDytG",
        "outputId": "22ec390a-1da5-4175-aabc-12388c47a9c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 및 검증 데이터 불러오기"
      ],
      "metadata": {
        "id": "-ve37vI1DcLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xBQEO6hD220",
        "outputId": "f323d0e3-10f3-44f6-a634-d1dc7395e619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 11:00:52--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.92.164.1\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.92.164.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-03-21 11:00:52 (13.7 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 압출 풀기\n",
        "tar_bz2_file = tarfile.open(\"/content/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=\"/content\")\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "NOLpAFRsFMnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 데이터 나누기"
      ],
      "metadata": {
        "id": "SNRMh4lmGcJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "\n",
        "train_length = int(len(data)*0.9)\n",
        "train = data[:train_length]\n",
        "vaild = data[train_length:]\n",
        "\n",
        "print('data_set:',len(data))\n",
        "print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "with open('train_split.json','w') as f:\n",
        "    json.dump(train,f,ensure_ascii = False)\n",
        "    \n",
        "with open('valid_split.json','w') as f:\n",
        "    json.dump(vaild,f,ensure_ascii = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w40eFDkFVB4",
        "outputId": "ee38b7ac-383e-4677-94ba-c98496a0daf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 코드 불러오기"
      ],
      "metadata": {
        "id": "RBtKjL3vDd-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 수정한 모델 소개\n",
        "\n",
        "모델의 FClayer 층이 참고하는 roberta model의 hidden states를 다각화 하여 성능을 향상시켰습니다.\n",
        "\n",
        "base model로 선정한 LostCow team의 FClayer층의 학습 방식은 roberta model이 문장A,B에 대해서 token_type_ids 값을 입력하지 않고 구분되지 않은 상태로 학습시키며 마지막 layer의 임베딩벡터를 문장A,B의 위치를 마스킹해둔 s1_mask, s2_mask를 이용하여 분리한뒤 각 단어들의 dim의 위치값을 평균내어 하나의 토큰 임베딩 값으로 생성한 후 FClayer를 통해 연산됩니다.\n",
        "\n",
        "이 때문에 roberta model의 pooled_output을 사용하지 않으며 사용시에도 성능 향상에 기여하지 못합니다.\n",
        "\n",
        "각 문장의 임베딩 벡터의 평균값을 기준으로 FClayer의 연산이 이루어지는 점을 이용하여 각 문장의 의미가 적당하게 고려된 hidden_states layer를 탐색하여 2,3layer의 hidden_states를 추가로 concat하는 것으로 모델의 성능을 향상시켰습니다.\n",
        "\n",
        "변경 전\n",
        "```python\n",
        "  -------------------base model(model.forward code)----------------------\n",
        "        outputs = self.roberta(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=None\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        s1_h = self.entity_average(sequence_output, s1_mask)\n",
        "        s2_h = self.entity_average(sequence_output, s2_mask)\n",
        "        s1_h = self.sentence_fc_layer(s1_h)\n",
        "        s2_h = self.sentence_fc_layer(s2_h)\n",
        "\n",
        "        concat_h = torch.cat([s1_h, s2_h], dim=-1)\n",
        "        concat_h = self.dense(concat_h)\n",
        "\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "```\n",
        "\n",
        "변경 후\n",
        "```python\n",
        "      ----------------custom model(model.forward code)---------------------\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=None, output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        s1_h = self.entity_average(sequence_output, s1_mask)\n",
        "        s2_h = self.entity_average(sequence_output, s2_mask)\n",
        "        s1_h = self.sentence_fc_layer(s1_h)\n",
        "        s2_h = self.sentence_fc_layer(s2_h)\n",
        "        \n",
        "        sequence_output_sec_layer=outputs['hidden_states'][2]\n",
        "        ss1_h = self.entity_average(sequence_output_sec_layer, s1_mask)\n",
        "        ss2_h = self.entity_average(sequence_output_sec_layer, s2_mask)\n",
        "        ss1_h = self.sentence_fc_layer2(ss1_h)\n",
        "        ss2_h = self.sentence_fc_layer2(ss2_h)\n",
        "\n",
        "        sequence_output_sec_layer2=outputs['hidden_states'][3]\n",
        "        sss1_h = self.entity_average(sequence_output_sec_layer2, s1_mask)\n",
        "        sss2_h = self.entity_average(sequence_output_sec_layer2, s2_mask)\n",
        "        sss1_h = self.sentence_fc_layer3(sss1_h)\n",
        "        sss2_h = self.sentence_fc_layer3(sss2_h)\n",
        "\n",
        "        concat_h = torch.cat([s1_h, s2_h,ss1_h, ss2_h,sss1_h, sss2_h], dim=-1)\n",
        "        concat_h = self.dense(concat_h)\n",
        "        concat_h = self.dense2(concat_h)\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + ()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "X2RhjdyEHocR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 파라미터 설명\n",
        "- `save_steps`을 100으로 설정한 것을 제외하곤 default 값으로 학습하였다.\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # data_arg\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"./data\")\n",
        "    parser.add_argument(\"--model_dir\", type=str, default=\"./model\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./output\")\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"klue/roberta-large\")\n",
        "    parser.add_argument(\"--train_filename\", type=str, default=\"klue-sts-v1.1_train.json\")\n",
        "    parser.add_argument(\"--valid_filename\", type=str, default=\"klue-sts-v1.1_dev.json\")\n",
        "\n",
        "    # train_arg\n",
        "    parser.add_argument(\"--num_labels\", type=int, default=1)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=10)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=110)\n",
        "\n",
        "    # eval_arg\n",
        "    parser.add_argument(\"--evaluation_strategy\", type=str, default=\"steps\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=250) # 100 steps\n",
        "    parser.add_argument(\"--save_total_limit\", type=int, default=2)\n",
        "\n",
        "    # wandb_log\n",
        "    parser.add_argument(\"--wandb_entity\", type=str, default='wanted_ai_06') # 수정 불필요\n",
        "    parser.add_argument(\"--wandb_project\", type=str, default='sohn_assign3') # 영어성씨_assign3로 수정할 것\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n",
        "```"
      ],
      "metadata": {
        "id": "VoRTOvOPIQ2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "h55QcBvPMAyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정한 모델 불러오기\n",
        "!git clone https://github.com/wanted-AI-06/Assignment-03"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g40eoYXzDZpS",
        "outputId": "5c308d28-d8e1-4db5-a4c4-9d0d2b0dc06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment-03'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 63 (delta 23), reused 44 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Assignment-03/code/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6AkE8gdFg4N",
        "outputId": "4b03a5ef-32b4-4456-9652-d6825d334d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwanted_ai_06\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220318_082354-3spy8l32\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdaily-tree-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/wanted_ai_06/sohn_assign3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/wanted_ai_06/sohn_assign3/runs/3spy8l32\u001b[0m\n",
            "Downloading: 100% 546/546 [00:00<00:00, 480kB/s]\n",
            "Downloading: 100% 375/375 [00:00<00:00, 300kB/s]\n",
            "Downloading: 100% 243k/243k [00:00<00:00, 1.31MB/s]\n",
            "Downloading: 100% 734k/734k [00:00<00:00, 2.91MB/s]\n",
            "Downloading: 100% 173/173 [00:00<00:00, 131kB/s]\n",
            "Downloading: 100% 422M/422M [00:09<00:00, 48.5MB/s]\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'sentence_fc_layer.linear.weight', 'cls_fc_layer.linear.bias', 'embedding_vectors.linear.bias', 'sentence_fc_layer2.linear.weight', 'sentence_fc_layer3.linear.bias', 'sentence_fc_layer2.linear.bias', 'dense.linear.bias', 'sentence_fc_layer3.linear.weight', 'dense2.linear.weight', 'cls_fc_layer.linear.weight', 'dense.linear.weight', 'sentence_fc_layer.linear.bias', 'label_classifier.linear.weight', 'dense2.linear.bias', 'label_classifier.linear.bias', 'roberta.pooler.dense.weight', 'embedding_vectors.linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "  6% 100/1650 [01:18<24:34,  1.05it/s]{'loss': 1.1387, 'learning_rate': 4.709090909090909e-05, 'epoch': 0.61}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.44it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.75it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.39it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.18it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.06it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  3.98it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.95it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.88it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.87it/s]\u001b[A\n",
            " 95% 18/19 [00:04<00:00,  3.87it/s]\u001b[A\n",
            "\n",
            "Downloading builder script: 3.85kB [00:00, 3.10MB/s]       \n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3972676992416382, 'eval_pearsonr': 0.950666728037494, 'eval_runtime': 5.3187, 'eval_samples_per_second': 219.414, 'eval_steps_per_second': 3.572, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:24<24:34,  1.05it/s]\n",
            "100% 19/19 [00:05<00:00,  3.87it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            " 12% 200/1650 [02:47<22:38,  1.07it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.2421, 'learning_rate': 4.406060606060606e-05, 'epoch': 1.21}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.42it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.71it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.36it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.19it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.08it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.96it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.92it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.91it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.90it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.90it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.366609662771225, 'eval_pearsonr': 0.9508796915045706, 'eval_runtime': 5.0859, 'eval_samples_per_second': 229.456, 'eval_steps_per_second': 3.736, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:52<22:38,  1.07it/s]\n",
            "100% 19/19 [00:04<00:00,  3.90it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.1754, 'learning_rate': 4.103030303030303e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:15<21:19,  1.05it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.45it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.71it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.37it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.18it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.08it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.00it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.97it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.90it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.88it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.87it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.86it/s]\u001b[A\n",
            "                                      \n",
            "{'eval_loss': 0.32836365699768066, 'eval_pearsonr': 0.9565973881218245, 'eval_runtime': 5.1018, 'eval_samples_per_second': 228.741, 'eval_steps_per_second': 3.724, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:20<21:19,  1.05it/s]\n",
            "100% 19/19 [00:04<00:00,  3.85it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            " 24% 400/1650 [05:43<19:59,  1.04it/s]{'loss': 0.1357, 'learning_rate': 3.8e-05, 'epoch': 2.42}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.41it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.70it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.38it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.21it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.07it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.00it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.96it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.91it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.89it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.88it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.86it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.35583218932151794, 'eval_pearsonr': 0.9547279046766726, 'eval_runtime': 5.105, 'eval_samples_per_second': 228.598, 'eval_steps_per_second': 3.722, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:48<19:59,  1.04it/s]\n",
            "100% 19/19 [00:04<00:00,  3.84it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            " 30% 500/1650 [07:11<17:37,  1.09it/s]{'loss': 0.1194, 'learning_rate': 3.4969696969696966e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [07:11<17:37,  1.09it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.75it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.48it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.76it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.41it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.23it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.11it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.94it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.91it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.87it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2424973100423813, 'eval_pearsonr': 0.9586758780788309, 'eval_runtime': 5.2329, 'eval_samples_per_second': 223.013, 'eval_steps_per_second': 3.631, 'epoch': 3.03}\n",
            " 30% 500/1650 [07:17<17:37,  1.09it/s]\n",
            "100% 19/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.093, 'learning_rate': 3.1939393939393944e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:40<16:28,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.45it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.73it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.38it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.20it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.09it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.95it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.92it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.91it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.90it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.28982171416282654, 'eval_pearsonr': 0.9595658120451372, 'eval_runtime': 5.0758, 'eval_samples_per_second': 229.916, 'eval_steps_per_second': 3.743, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:45<16:28,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.90it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.083, 'learning_rate': 2.8909090909090908e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [10:08<14:52,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.79it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.45it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.72it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.37it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.19it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.09it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.99it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.97it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.95it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.93it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.88it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2540854811668396, 'eval_pearsonr': 0.9587223138658242, 'eval_runtime': 5.0748, 'eval_samples_per_second': 229.959, 'eval_steps_per_second': 3.744, 'epoch': 4.24}\n",
            " 42% 700/1650 [10:13<14:52,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            " 48% 800/1650 [11:36<13:12,  1.07it/s]{'loss': 0.0677, 'learning_rate': 2.587878787878788e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:36<13:12,  1.07it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.45it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.72it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.38it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.20it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.08it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.01it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.96it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.91it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2683432102203369, 'eval_pearsonr': 0.9595898286179911, 'eval_runtime': 5.1366, 'eval_samples_per_second': 227.192, 'eval_steps_per_second': 3.699, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:41<13:12,  1.07it/s]\n",
            "100% 19/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            " 55% 900/1650 [13:03<11:50,  1.06it/s]{'loss': 0.0577, 'learning_rate': 2.284848484848485e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [13:03<11:50,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.48it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.75it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.42it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.22it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.10it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.97it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.90it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.90it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                      \n",
            "                                   {'eval_loss': 0.2500363886356354, 'eval_pearsonr': 0.9601290500078997, 'eval_runtime': 5.237, 'eval_samples_per_second': 222.837, 'eval_steps_per_second': 3.628, 'epoch': 5.45}\n",
            " 55% 900/1650 [13:09<11:50,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.054, 'learning_rate': 1.981818181818182e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [14:31<10:14,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.46it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.75it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.41it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.22it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.10it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.03it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2727000415325165, 'eval_pearsonr': 0.9586247268368471, 'eval_runtime': 5.0981, 'eval_samples_per_second': 228.908, 'eval_steps_per_second': 3.727, 'epoch': 6.06}\n",
            " 61% 1000/1650 [14:36<10:14,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            " 67% 1100/1650 [15:59<08:35,  1.07it/s]{'loss': 0.0434, 'learning_rate': 1.6787878787878787e-05, 'epoch': 6.67}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.80it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.50it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.77it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.43it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.22it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.09it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  3.99it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.96it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.85it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.86it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21082162857055664, 'eval_pearsonr': 0.9625535705769546, 'eval_runtime': 5.0992, 'eval_samples_per_second': 228.858, 'eval_steps_per_second': 3.726, 'epoch': 6.67}\n",
            " 67% 1100/1650 [16:04<08:35,  1.07it/s]\n",
            "100% 19/19 [00:04<00:00,  3.86it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            " 73% 1200/1650 [17:27<07:06,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.0425, 'learning_rate': 1.3757575757575758e-05, 'epoch': 7.27}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.67it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.47it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.75it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.37it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.18it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.07it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.01it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.95it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.85it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.81it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.81it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.81it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.80it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.80it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.77it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.75it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.22362491488456726, 'eval_pearsonr': 0.959672129563923, 'eval_runtime': 5.1864, 'eval_samples_per_second': 225.013, 'eval_steps_per_second': 3.663, 'epoch': 7.27}\n",
            " 73% 1200/1650 [17:32<07:06,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.76it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            " 79% 1300/1650 [18:57<05:27,  1.07it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.0361, 'learning_rate': 1.0727272727272727e-05, 'epoch': 7.88}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.80it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.49it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.74it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.40it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.21it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.09it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.02it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.94it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.92it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.90it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.87it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.87it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.87it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23936671018600464, 'eval_pearsonr': 0.9607902470472457, 'eval_runtime': 5.2171, 'eval_samples_per_second': 223.686, 'eval_steps_per_second': 3.642, 'epoch': 7.88}\n",
            " 79% 1300/1650 [19:02<05:27,  1.07it/s]\n",
            "100% 19/19 [00:04<00:00,  3.88it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            " 85% 1400/1650 [20:25<03:55,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.0315, 'learning_rate': 7.696969696969696e-06, 'epoch': 8.48}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.70it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.45it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.73it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.38it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.17it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.06it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  3.99it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.94it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.91it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.90it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.89it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.87it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.85it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.22051690518856049, 'eval_pearsonr': 0.9617835242966644, 'eval_runtime': 5.1098, 'eval_samples_per_second': 228.385, 'eval_steps_per_second': 3.718, 'epoch': 8.48}\n",
            " 85% 1400/1650 [20:30<03:55,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.85it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
            " 91% 1500/1650 [21:53<02:21,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.0284, 'learning_rate': 4.666666666666667e-06, 'epoch': 9.09}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.80it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.50it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.74it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.39it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.21it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.09it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.02it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.98it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.95it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.93it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.91it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.91it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.90it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.22420917451381683, 'eval_pearsonr': 0.960997511103577, 'eval_runtime': 5.0957, 'eval_samples_per_second': 229.018, 'eval_steps_per_second': 3.729, 'epoch': 9.09}\n",
            " 91% 1500/1650 [21:58<02:21,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            " 97% 1600/1650 [23:22<00:47,  1.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "{'loss': 0.0257, 'learning_rate': 1.6363636363636367e-06, 'epoch': 9.7}\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:02,  7.76it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  5.49it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:03,  4.75it/s]\u001b[A\n",
            " 26% 5/19 [00:01<00:03,  4.40it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:03,  4.22it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.11it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.02it/s]\u001b[A\n",
            " 47% 9/19 [00:02<00:02,  3.97it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  3.94it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:02,  3.91it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  3.90it/s]\u001b[A\n",
            " 68% 13/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 74% 14/19 [00:03<00:01,  3.89it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:01,  3.88it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  3.89it/s]\u001b[A\n",
            " 89% 17/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24611248075962067, 'eval_pearsonr': 0.9609050341621567, 'eval_runtime': 5.102, 'eval_samples_per_second': 228.735, 'eval_steps_per_second': 3.724, 'epoch': 9.7}\n",
            " 97% 1600/1650 [23:27<00:47,  1.06it/s]\n",
            "100% 19/19 [00:04<00:00,  3.89it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [24:10<00:00,  1.63it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1100 (score: 0.9625535705769546).\n",
            "                                       {'train_runtime': 1451.2848, 'train_samples_per_second': 72.357, 'train_steps_per_second': 1.137, 'train_loss': 0.14460775686032845, 'epoch': 10.0}\n",
            "100% 1650/1650 [24:11<00:00,  1.14it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▇▅▆▂▄▃▃▂▃▁▁▂▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/pearsonr ▁▁▄▃▆▆▆▆▇▆█▆▇█▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime █▁▂▂▆▁▁▃▆▂▂▄▅▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁█▇▇▃██▆▃▇▇▅▄▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁█▇▇▃██▆▃▇▇▅▄▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.24611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/pearsonr 0.96091\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 5.102\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 228.735\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 3.724\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 10.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1650\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.0257\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6385965707893200.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.14461\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1451.2848\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 72.357\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdaily-tree-21\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/wanted_ai_06/sohn_assign3/runs/3spy8l32\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220318_082354-3spy8l32/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[WandB 로깅 기록 바로가기](https://wandb.ai/wanted_ai_06/sohn_assign3/runs/3spy8l32?workspace=user-arc)"
      ],
      "metadata": {
        "id": "TtYHCUugKxyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 테스트\n"
      ],
      "metadata": {
        "id": "277zYVDJGbIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/klue-sts-v1.1.tar.gz /content/model/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "id": "UdwEwMRlNPPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/Assignment-03/code/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "rQrt5TtqGDak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valid의 label 값을 추출\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 후 혼동 행렬 및 f1_score 시각화\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base) f1_score:',f1_score(valid_label, pred), end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>3).astype(int)\n",
        "print(\"epochs 10 / >3\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('custom_model(roberta-base) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rck0PPfDMIW1",
        "outputId": "35cbaa84-29b3-4bb0-8bb2-a3d38a36cb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.69      0.81       299\n",
            "           1       0.70      1.00      0.82       220\n",
            "\n",
            "    accuracy                           0.82       519\n",
            "   macro avg       0.85      0.84      0.82       519\n",
            "weighted avg       0.87      0.82      0.82       519\n",
            "\n",
            "custom_model(roberta-base) f1_score: 0.8217636022514071\n",
            "\n",
            "epochs 10 / >3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.79      0.86       299\n",
            "           1       0.77      0.95      0.85       220\n",
            "\n",
            "    accuracy                           0.86       519\n",
            "   macro avg       0.86      0.87      0.86       519\n",
            "weighted avg       0.87      0.86      0.86       519\n",
            "\n",
            "custom_model(roberta-base) f1_score: 0.8478701825557808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dev_set에 대한 pearsonr score 출력 \n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PQwca9tMUCT",
        "outputId": "acf5c05c-1529-49f9-c9f3-290564882564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8869273898083346}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 성능 비교\n",
        "\n",
        "## 성능 비교 표\n",
        "|모델 특징|전처리 유무|dev peasonr score|dev f1 score|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|base|X|~ 0.894|~ 0.859|\n",
        "|custom|X|0.887 ~ 0.902|0.848 ~ 0.870|\n",
        "|custom|영어, 숫자, 특수문자 전처리|0.847|0.835|\n",
        "|custom|영어만 소문자 처리|0.808|0.823|\n",
        "\n",
        "전처리를 통한 성능 감소를 확인 하였고 전처리를 진행하지 않고 학습시킨 custom 모델을 최종 선정하였습니다.\n",
        "모델 튜닝 결과, 피어슨 상관계수 0.902, F1 score가 0.870를 달성하였다."
      ],
      "metadata": {
        "id": "C6R1FF5sMZCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 최고 성능 모델 "
      ],
      "metadata": {
        "id": "E1JW2i9nIWwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dev_set에 대한 pearsonr score 출력 \n",
        "file_path=\"dev_set_score.csv\"\n",
        "df=pd.read_csv(file_path)\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "print(classification_report(df['true_binary_label'], df['predict_binary_label']))\n",
        "print('custom(roberta-base) f1_score:',f1_score(df['true_binary_label'], df['predict_binary_label']),end='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbwFWM2Iddg",
        "outputId": "cf9b5a0f-e9b7-4d03-80f2-905d0461789f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.82      0.89       299\n",
            "           1       0.79      0.96      0.87       220\n",
            "\n",
            "    accuracy                           0.88       519\n",
            "   macro avg       0.88      0.89      0.88       519\n",
            "weighted avg       0.89      0.88      0.88       519\n",
            "\n",
            "custom(roberta-base) f1_score: 0.8706365503080084\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "pearson(predictions=df['predict_real_label'], references=df['true_real_label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Pr-7Z8JmoE",
        "outputId": "c6237712-5321-4d7f-bac9-f8bb95bccb80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearsonr': 0.9020511636828425}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}
