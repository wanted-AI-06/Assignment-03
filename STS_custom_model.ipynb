{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STS_custom_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모델(roberta-base)의 custom 및 f1_score & pearsonr \n",
        "\n",
        "Team LostCow의 roberta모델을 기준으로 함\n",
        "\n",
        "출처: https://github.com/l-yohai/KLUE"
      ],
      "metadata": {
        "id": "pmsQRpsmRRfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez-KaYpLFd04",
        "outputId": "58ad612d-7305-49e6-f6e8-9debafffa1b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 73.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 72.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 73.7 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "#훈련 데이터셋 다운로드 및 압축풀기\n",
        "\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
        "\n",
        "tar_bz2_file = tarfile.open(\"/content/klue-sts-v1.1.tar.gz\")\n",
        "tar_bz2_file.extractall(path=\"/content\")\n",
        "tar_bz2_file.close()"
      ],
      "metadata": {
        "id": "3J5pSUgIHv0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b20fc03-4a67-4868-95a2-56ab046dc6b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 06:28:46--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.92.165.217\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.92.165.217|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1349881 (1.3M) [application/x-gzip]\n",
            "Saving to: ‘klue-sts-v1.1.tar.gz’\n",
            "\n",
            "klue-sts-v1.1.tar.g 100%[===================>]   1.29M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-03-17 06:28:46 (17.8 MB/s) - ‘klue-sts-v1.1.tar.gz’ saved [1349881/1349881]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#GPU 사용 여부 확인 및 name 확인\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "oZoPB1WKJnet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1287ef3a-cb44-4879-f15c-9c5345998362"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jy5VuDP_w1T",
        "outputId": "99ec94fc-1cb5-4bc3-a9e2-e6a576d6399a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_set: 11668\n",
            "train_set: 10501 , valid_set: 1167\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#train 데이터셋을 train,valid 데이터셋으로 split\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "\n",
        "train_length=int(len(data)*0.9)\n",
        "train=data[:train_length]\n",
        "vaild=data[train_length:]\n",
        "\n",
        "print('data_set:',len(data))\n",
        "print('train_set:',len(train),', valid_set:',len(vaild))\n",
        "\n",
        "with open('train_split.json','w') as f:\n",
        "  json.dump(train,f,ensure_ascii = False)\n",
        "with open('valid_split.json','w') as f:\n",
        "  json.dump(vaild,f,ensure_ascii = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 custom을 진행한 폴더 압축해제\n",
        "!unzip /content/KLUE_custom.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7hnyZVgAuf2",
        "outputId": "18b40df7-da16-4818-b1d2-f43dea9a41d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/KLUE_custom.zip\n",
            "  inflating: sts/__pycache__/dataset.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/metric.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/model.cpython-37.pyc  \n",
            "  inflating: sts/__pycache__/utils.cpython-37.pyc  \n",
            "  inflating: sts/data/download.sh    \n",
            "  inflating: sts/README.md           \n",
            "  inflating: sts/dataloader.py       \n",
            "  inflating: sts/dataset.py          \n",
            "  inflating: sts/infer_test.ipynb    \n",
            "  inflating: sts/inference.py        \n",
            "  inflating: sts/metric.py           \n",
            "  inflating: sts/model.py            \n",
            " extracting: sts/requirements.txt    \n",
            "  inflating: sts/train.py            \n",
            "  inflating: sts/utils.py            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom 모델 학습\n",
        "!python /content/sts/train.py --data_dir /content --model_name_or_path \"klue/roberta-base\" --train_filename \"train_split.json\" --valid_filename \"valid_split.json\" --num_train_epochs 10 --save_steps 100 --save_steps 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvAb8SjFdMR",
        "outputId": "14063a37-d399-45ae-9591-dfd0082526e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForStsRegression: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForStsRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForStsRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForStsRegression were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['sentence_fc_layer3.linear.weight', 'sentence_fc_layer2.linear.bias', 'roberta.pooler.dense.weight', 'sentence_fc_layer2.linear.weight', 'embedding_vectors.linear.weight', 'embedding_vectors.linear.bias', 'sentence_fc_layer3.linear.bias', 'sentence_fc_layer.linear.bias', 'dense.linear.bias', 'dense.linear.weight', 'dense2.linear.weight', 'label_classifier.linear.bias', 'label_classifier.linear.weight', 'roberta.pooler.dense.bias', 'cls_fc_layer.linear.weight', 'cls_fc_layer.linear.bias', 'sentence_fc_layer.linear.weight', 'dense2.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1650\n",
            "{'loss': 1.147, 'learning_rate': 4.706060606060606e-05, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:14<19:19,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2812851369380951, 'eval_pearsonr': 0.9504658125624138, 'eval_runtime': 4.5251, 'eval_samples_per_second': 257.893, 'eval_steps_per_second': 4.199, 'epoch': 0.61}\n",
            "  6% 100/1650 [01:19<19:19,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-100\n",
            "Configuration saved in ./model/checkpoint-100/config.json\n",
            "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
            "{'loss': 0.259, 'learning_rate': 4.403030303030303e-05, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:38<18:04,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.3573341965675354, 'eval_pearsonr': 0.9539752101861693, 'eval_runtime': 4.5204, 'eval_samples_per_second': 258.165, 'eval_steps_per_second': 4.203, 'epoch': 1.21}\n",
            " 12% 200/1650 [02:43<18:04,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-200\n",
            "Configuration saved in ./model/checkpoint-200/config.json\n",
            "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
            "{'loss': 0.1899, 'learning_rate': 4.1e-05, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:02<16:49,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2156623899936676, 'eval_pearsonr': 0.9578964923931891, 'eval_runtime': 4.5263, 'eval_samples_per_second': 257.824, 'eval_steps_per_second': 4.198, 'epoch': 1.82}\n",
            " 18% 300/1650 [04:07<16:49,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-300\n",
            "Configuration saved in ./model/checkpoint-300/config.json\n",
            "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 0.1428, 'learning_rate': 3.796969696969697e-05, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:26<15:34,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.68it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.46it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2566785216331482, 'eval_pearsonr': 0.9588265095306583, 'eval_runtime': 4.6852, 'eval_samples_per_second': 249.084, 'eval_steps_per_second': 4.055, 'epoch': 2.42}\n",
            " 24% 400/1650 [05:31<15:34,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-400\n",
            "Configuration saved in ./model/checkpoint-400/config.json\n",
            "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.1239, 'learning_rate': 3.4939393939393935e-05, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:50<13:41,  1.40it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.42948052287101746, 'eval_pearsonr': 0.9543794025820584, 'eval_runtime': 4.5172, 'eval_samples_per_second': 258.343, 'eval_steps_per_second': 4.206, 'epoch': 3.03}\n",
            " 30% 500/1650 [06:54<13:41,  1.40it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-500\n",
            "Configuration saved in ./model/checkpoint-500/config.json\n",
            "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
            "{'loss': 0.0987, 'learning_rate': 3.190909090909091e-05, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:14<13:04,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.38384419679641724, 'eval_pearsonr': 0.9575950454757214, 'eval_runtime': 4.5219, 'eval_samples_per_second': 258.08, 'eval_steps_per_second': 4.202, 'epoch': 3.64}\n",
            " 36% 600/1650 [08:19<13:04,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-600\n",
            "Configuration saved in ./model/checkpoint-600/config.json\n",
            "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 0.0918, 'learning_rate': 2.8878787878787884e-05, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:38<11:50,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.69it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.14it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2663321793079376, 'eval_pearsonr': 0.9610383173400803, 'eval_runtime': 4.5446, 'eval_samples_per_second': 256.789, 'eval_steps_per_second': 4.181, 'epoch': 4.24}\n",
            " 42% 700/1650 [09:43<11:50,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-700\n",
            "Configuration saved in ./model/checkpoint-700/config.json\n",
            "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 0.0714, 'learning_rate': 2.584848484848485e-05, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:02<10:34,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.71it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.30818426609039307, 'eval_pearsonr': 0.9569026931559972, 'eval_runtime': 4.6492, 'eval_samples_per_second': 251.012, 'eval_steps_per_second': 4.087, 'epoch': 4.85}\n",
            " 48% 800/1650 [11:07<10:34,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-800\n",
            "Configuration saved in ./model/checkpoint-800/config.json\n",
            "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 0.0663, 'learning_rate': 2.281818181818182e-05, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:26<09:20,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.2163790762424469, 'eval_pearsonr': 0.958863478540369, 'eval_runtime': 4.5254, 'eval_samples_per_second': 257.877, 'eval_steps_per_second': 4.199, 'epoch': 5.45}\n",
            " 55% 900/1650 [12:31<09:20,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-900\n",
            "Configuration saved in ./model/checkpoint-900/config.json\n",
            "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 0.0625, 'learning_rate': 1.978787878787879e-05, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:50<08:02,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.62it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24268914759159088, 'eval_pearsonr': 0.9616619171869646, 'eval_runtime': 4.5393, 'eval_samples_per_second': 257.087, 'eval_steps_per_second': 4.186, 'epoch': 6.06}\n",
            " 61% 1000/1650 [13:54<08:02,  1.35it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1000\n",
            "Configuration saved in ./model/checkpoint-1000/config.json\n",
            "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0481, 'learning_rate': 1.6757575757575757e-05, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:14<06:50,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.73it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.52it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2557995617389679, 'eval_pearsonr': 0.9609723117019314, 'eval_runtime': 4.5388, 'eval_samples_per_second': 257.114, 'eval_steps_per_second': 4.186, 'epoch': 6.67}\n",
            " 67% 1100/1650 [15:19<06:50,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1100\n",
            "Configuration saved in ./model/checkpoint-1100/config.json\n",
            "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
            "{'loss': 0.0445, 'learning_rate': 1.3727272727272727e-05, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:38<05:36,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.96it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.36it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.30647116899490356, 'eval_pearsonr': 0.9600131903462908, 'eval_runtime': 4.6608, 'eval_samples_per_second': 250.386, 'eval_steps_per_second': 4.077, 'epoch': 7.27}\n",
            " 73% 1200/1650 [16:43<05:36,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1200\n",
            "Configuration saved in ./model/checkpoint-1200/config.json\n",
            "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 0.0389, 'learning_rate': 1.0696969696969696e-05, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:03<04:21,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.74it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.15it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.38it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.23693805932998657, 'eval_pearsonr': 0.9629029820765507, 'eval_runtime': 4.5254, 'eval_samples_per_second': 257.877, 'eval_steps_per_second': 4.199, 'epoch': 7.88}\n",
            " 79% 1300/1650 [18:07<04:21,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1300\n",
            "Configuration saved in ./model/checkpoint-1300/config.json\n",
            "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 0.0343, 'learning_rate': 7.666666666666667e-06, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:26<03:06,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.34it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2718634307384491, 'eval_pearsonr': 0.9603407575505541, 'eval_runtime': 4.5273, 'eval_samples_per_second': 257.771, 'eval_steps_per_second': 4.197, 'epoch': 8.48}\n",
            " 85% 1400/1650 [19:31<03:06,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.36it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1400\n",
            "Configuration saved in ./model/checkpoint-1400/config.json\n",
            "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0329, 'learning_rate': 4.636363636363636e-06, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:50<01:51,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.95it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.74it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.61it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.48it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.44it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.42it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.40it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.39it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.37it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.26364871859550476, 'eval_pearsonr': 0.9604865068804863, 'eval_runtime': 4.518, 'eval_samples_per_second': 258.298, 'eval_steps_per_second': 4.205, 'epoch': 9.09}\n",
            " 91% 1500/1650 [20:55<01:51,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.37it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1500\n",
            "Configuration saved in ./model/checkpoint-1500/config.json\n",
            "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 0.0281, 'learning_rate': 1.606060606060606e-06, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:15<00:37,  1.34it/s]***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:00<00:01,  8.72it/s]\u001b[A\n",
            " 16% 3/19 [00:00<00:02,  6.16it/s]\u001b[A\n",
            " 21% 4/19 [00:00<00:02,  5.33it/s]\u001b[A\n",
            " 26% 5/19 [00:00<00:02,  4.94it/s]\u001b[A\n",
            " 32% 6/19 [00:01<00:02,  4.73it/s]\u001b[A\n",
            " 37% 7/19 [00:01<00:02,  4.60it/s]\u001b[A\n",
            " 42% 8/19 [00:01<00:02,  4.53it/s]\u001b[A\n",
            " 47% 9/19 [00:01<00:02,  4.47it/s]\u001b[A\n",
            " 53% 10/19 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 58% 11/19 [00:02<00:01,  4.41it/s]\u001b[A\n",
            " 63% 12/19 [00:02<00:01,  4.38it/s]\u001b[A\n",
            " 68% 13/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 74% 14/19 [00:02<00:01,  4.37it/s]\u001b[A\n",
            " 79% 15/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 84% 16/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            " 89% 17/19 [00:03<00:00,  4.36it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.25988274812698364, 'eval_pearsonr': 0.9606646058304802, 'eval_runtime': 4.6611, 'eval_samples_per_second': 250.37, 'eval_steps_per_second': 4.076, 'epoch': 9.7}\n",
            " 97% 1600/1650 [22:19<00:37,  1.34it/s]\n",
            "100% 19/19 [00:04<00:00,  4.35it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./model/checkpoint-1600\n",
            "Configuration saved in ./model/checkpoint-1600/config.json\n",
            "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
            "100% 1650/1650 [23:01<00:00,  1.80it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./model/checkpoint-1300 (score: 0.9629029820765507).\n",
            "{'train_runtime': 1383.7595, 'train_samples_per_second': 75.887, 'train_steps_per_second': 1.192, 'train_loss': 0.15112074649695195, 'epoch': 10.0}\n",
            "100% 1650/1650 [23:03<00:00,  1.19it/s]\n",
            "Configuration saved in ./model/config.json\n",
            "Model weights saved in ./model/pytorch_model.bin\n",
            "tokenizer config file saved in ./model/tokenizer_config.json\n",
            "Special tokens file saved in ./model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습한 모델을 이용하여 dev_set 예측값 출력(real_label)\n",
        "!python /content/sts/inference.py --test_filename \"/content/klue-sts-v1.1/klue-sts-v1.1_dev.json\" --output_dir \"/content\"  --model_tar_file \"klue-sts-v1.1.tar.gz\""
      ],
      "metadata": {
        "id": "P_WMvlynKy3S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,f1_score\n",
        "import pandas as pd\n",
        "\n",
        "#valid의 label 값을 추출\n",
        "\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['binary-label'] for data in valid_data]\n",
        "\n",
        "\n",
        "#Regression pred 를 binary-label pred로 변환 및 혼돈 메트릭스 및 f1_score 시각화\n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "\n",
        "pred=(df_pred>2.5).astype(int)\n",
        "print(\"epochs 10 / >2.5\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred),end='\\n\\n')\n",
        "\n",
        "pred=(df_pred>2.85).astype(int)\n",
        "print(\"epochs 10 / >2.85\")\n",
        "print(classification_report(valid_label, pred))\n",
        "print('base_line(roberta-base) f1_score:',f1_score(valid_label, pred))"
      ],
      "metadata": {
        "id": "lof2YeveXdBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497e3306-a6e1-4c5c-fdb2-7ada4384e2d8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs 10 / >2.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.71      0.83       299\n",
            "           1       0.71      0.99      0.83       220\n",
            "\n",
            "    accuracy                           0.83       519\n",
            "   macro avg       0.85      0.85      0.83       519\n",
            "weighted avg       0.87      0.83      0.83       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8304761904761905\n",
            "\n",
            "epochs 10 / >2.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.80      0.88       299\n",
            "           1       0.78      0.97      0.86       220\n",
            "\n",
            "    accuracy                           0.87       519\n",
            "   macro avg       0.88      0.88      0.87       519\n",
            "weighted avg       0.89      0.87      0.87       519\n",
            "\n",
            "base_line(roberta-base) f1_score: 0.8640973630831643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "#dev_set에 대한 pearsonr score 출력 \n",
        "\n",
        "df_pred = pd.read_csv('/content/output.csv',header=None)\n",
        "valid_data=read_json('/content/klue-sts-v1.1/klue-sts-v1.1_dev.json')\n",
        "valid_label=[data['labels']['real-label'] for data in valid_data]\n",
        "\n",
        "pearson = load_metric(\"pearsonr\").compute\n",
        "metric = pearson(predictions=df_pred.to_numpy(), references=valid_label)\n",
        "\n",
        "print(metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZOYVIlAsqWd",
        "outputId": "b668f84a-383f-48b2-8f91-084138527bf4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pearsonr': 0.8981787692339827}\n"
          ]
        }
      ]
    }
  ]
}